{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a97fb07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Softplus\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#datasets.mkdir_p('img_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6f359c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent = 50\n",
    "num_bins = 51\n",
    "num_neurons = [1000, 1000]\n",
    "batch_size = 50\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "load_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "159fb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b35a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cb60ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "469\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b7d834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of example: 60000\n",
      "dimension of flat image: 784\n"
     ]
    }
   ],
   "source": [
    "train_N = len(train_dataset)\n",
    "\n",
    "first_img, _ = train_dataset[0]\n",
    "train_D = first_img.numel() \n",
    "\n",
    "print(f\"number of example: {train_N}\")\n",
    "print(f\"dimension of flat image: {train_D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fad1a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, in_channels: int = 1, n_conv_blocks: int = 2, base_filters: int = 32):\n",
    "        \"\"\"\n",
    "        Encoder che usa convoluzioni e restituisce media e logvar per la distribuzione latente q(z|x)\n",
    "        \n",
    "        Args:\n",
    "            n_latent (int): dimensione del vettore latente Z\n",
    "            in_channels (int): numero di canali dell'immagine (1 per MNIST)\n",
    "            n_conv_blocks (int): numero di blocchi convoluzionali\n",
    "            base_filters (int): numero iniziale di filtri (verrà raddoppiato a ogni blocco)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        filters = base_filters\n",
    "\n",
    "        # Costruzione dei blocchi convoluzionali\n",
    "        for _ in range(n_conv_blocks):\n",
    "            layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(filters))\n",
    "            layers.append(nn.ELU(inplace=True))\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        # Calcola dimensione dell'output convoluzionale\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 28, 28)  # immagine finta MNIST\n",
    "            conv_out = self.conv(dummy_input)\n",
    "            self.flattened_size = conv_out.view(1, -1).shape[1]\n",
    "\n",
    "        # Strato finale che mappa su media e logvar del vettore latente\n",
    "        self.fc_mu = nn.Linear(self.flattened_size, n_latent)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_size, n_latent)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8978a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, output_channels: int = 1, n_conv_blocks: int = 2, base_filters: int = 32):\n",
    "        \"\"\"\n",
    "        Decoder che prende un vettore latente z e ricostruisce un'immagine MNIST (28x28).\n",
    "        \n",
    "        Args:\n",
    "            n_latent (int): dimensione del vettore latente z\n",
    "            output_channels (int): numero di canali dell'immagine in output (1 per MNIST)\n",
    "            n_conv_blocks (int): numero di blocchi deconvoluzionali\n",
    "            base_filters (int): numero iniziale di filtri (decrescente durante il decoding)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_conv_blocks = n_conv_blocks\n",
    "        self.base_filters = base_filters\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        # Calcolo della dimensione della feature map dopo l'encoder (dipende dalla struttura simmetrica)\n",
    "        self.init_spatial_dim = 28 // (2 ** n_conv_blocks)  # per MNIST 28x28\n",
    "        self.init_filters = base_filters * (2 ** (n_conv_blocks - 1))\n",
    "        self.projected_dim = self.init_filters * self.init_spatial_dim * self.init_spatial_dim\n",
    "\n",
    "        # Proiezione lineare dal vettore latente alla mappa iniziale\n",
    "        self.fc = nn.Linear(n_latent, self.projected_dim)\n",
    "\n",
    "        # Costruzione dei blocchi trasposti\n",
    "        layers = []\n",
    "        filters = self.init_filters\n",
    "        for i in range(n_conv_blocks - 1):\n",
    "            layers.append(nn.ConvTranspose2d(filters, filters // 2, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(nn.BatchNorm2d(filters // 2))\n",
    "            layers.append(nn.ELU(inplace=True))\n",
    "            filters //= 2\n",
    "\n",
    "        # Ultimo blocco: output a 1 canale (immagine ricostruita)\n",
    "        layers.append(nn.ConvTranspose2d(filters, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        layers.append(nn.Sigmoid())  # output in [0, 1]\n",
    "\n",
    "        self.deconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), self.init_filters, self.init_spatial_dim, self.init_spatial_dim)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd589a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.decoder = decoder.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        VAE: encode → reparametrize → decode.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "         z = mu + sigma * epsilon\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        return (mu, logvar)\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Reconstruct the image starting from latent vector z\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        \"\"\"\n",
    "        encode → reparametrize → decode\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z)\n",
    "\n",
    "    def sample(self, n_samples: int):\n",
    "        \"\"\"\n",
    "        sample z ~ N(0,I) .\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.encoder.n_latent, device=self.device)\n",
    "            samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def elbo(self, x, beta=1.0):\n",
    "        \"\"\"\n",
    "        Compute ELBO\n",
    "        \"\"\"\n",
    "        x_recon, mu, logvar = self.forward(x)\n",
    "\n",
    "        # Ricostruzione con MSE o BCE\n",
    "        recon_loss = nn.functional.binary_cross_entropy(\n",
    "            x_recon, x, reduction='sum'\n",
    "        )\n",
    "\n",
    "        # KL divergente tra N(mu, sigma^2) e N(0, 1)\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        elbo = recon_loss + beta * kl_div\n",
    "        return elbo, recon_loss, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce7bdd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_loss(mu_z, var_z):\n",
    "    return torch.mean(0.5 * torch.sum(mu_z**2 + var_z**2 - torch.log(var_z**2) - 1, dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "93a0bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear, ELU, BatchNorm2d, Conv2d, ModuleList, Softplus\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GaussianEncoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, in_channels: int = 1, n_conv_blocks: int = 3, n_filters: int = 32, n_fc: int = 2048):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.n_latent = n_latent\n",
    "\n",
    "        # Build convolutional encoder\n",
    "        layers = []\n",
    "        for _ in range(n_conv_blocks):\n",
    "            layers.append(Conv2d(in_channels, n_filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(BatchNorm2d(n_filters))\n",
    "            layers.append(ELU(inplace=True))\n",
    "            in_channels = n_filters\n",
    "            n_filters *= 2\n",
    "        self.conv_head = ModuleList(layers)\n",
    "\n",
    "        # Linear layers to latent space\n",
    "        self.flattened_size = n_fc\n",
    "        self.mu = Linear(n_fc, n_latent)\n",
    "        self.var = Linear(n_fc, n_latent)\n",
    "        self.var_act = Softplus()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.conv_head:\n",
    "            out = layer(out)\n",
    "        out = out.flatten(start_dim=1)  # shape: [batch, flattened]\n",
    "        mu = self.mu(out)\n",
    "        var = self.var_act(self.var(out))\n",
    "        z = self.reparameterize(mu, var)\n",
    "        return z, mu, var\n",
    "\n",
    "    def reparameterize(self, mu, var):\n",
    "        eps = torch.randn_like(var)\n",
    "        return mu + eps * torch.sqrt(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ccf2e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear, ELU, BatchNorm2d, ConvTranspose2d, Sequential\n",
    "from torch.distributions import Normal\n",
    "import math\n",
    "\n",
    "class GaussianDecoder(nn.Module):\n",
    "    def __init__(self, n_pixels: int, n_latent: int, n_deconv_blocks: int = 3, init_filters: int = 128, n_fc: int = 2048, var: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.var = var\n",
    "\n",
    "        self.n_fc = n_fc\n",
    "        self.init_filters = init_filters\n",
    "        self.deconv_input_size = int(math.sqrt(n_fc // init_filters))\n",
    "\n",
    "        # MLP to project latent space to feature map\n",
    "        self.linear = Linear(n_latent, n_fc)\n",
    "\n",
    "        # Decoder: series of ConvTranspose2d to reconstruct 28x28\n",
    "        filters = init_filters\n",
    "        layers = []\n",
    "\n",
    "        for _ in range(n_deconv_blocks - 1):\n",
    "            layers.append(ConvTranspose2d(filters, filters // 2, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(BatchNorm2d(filters // 2))\n",
    "            layers.append(ELU(inplace=True))\n",
    "            filters //= 2\n",
    "\n",
    "        # Final layer: output one channel for MNIST\n",
    "        layers.append(ConvTranspose2d(filters, 1, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        self.decoder = Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.linear(z)\n",
    "        out = out.view(z.size(0), self.init_filters, self.deconv_input_size, self.deconv_input_size)\n",
    "        out = self.decoder(out)\n",
    "        return out  # mu\n",
    "\n",
    "    def output(self, deconvoluted):\n",
    "        return deconvoluted  # Already mu\n",
    "\n",
    "    def reconstruct(self, z):\n",
    "        mu = self.forward(z)\n",
    "        x_hat = Normal(mu, self.var).rsample()  # Use rsample for autograd compatibility\n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "01471969",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianVAE(nn.Module):\n",
    "    def __init__(self, n_latent: int, input_shape: tuple, var: float = 0.05):\n",
    "        super().__init__()\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.n_latent = n_latent\n",
    "        self.var = var  # varianza del decoder\n",
    "\n",
    "        self.encoder = GaussianEncoder(n_latent).to(self.device)\n",
    "        self.decoder = GaussianDecoder(input_shape, n_latent, var=var).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, _, _ = self.encode(x)\n",
    "        return self.decode(z)\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def elbo(self, x):\n",
    "        z, mu_z, var_z = self.encode(x)\n",
    "        x_recon = self.decode(z)\n",
    "\n",
    "        # Ricostruzione: calcolo MSE tra immagine originale e decodificata\n",
    "        mse = (x_recon - x) ** 2\n",
    "        recon_loss = 0.5 * torch.sum(torch.log(torch.tensor(2 * np.pi * self.var)) + mse / self.var)\n",
    "\n",
    "        # KL Divergence\n",
    "        kl = kl_loss(mu_z, var_z)\n",
    "\n",
    "        return recon_loss + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "54274070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vae_params(vae: VAE):\n",
    "    print(vae_gaussian.encoder)\n",
    "    print(vae_gaussian.decoder)\n",
    "    print(f'Number of encoder parameters: {sum(p.numel() for p in vae_gaussian.encoder.parameters())}')\n",
    "    print(f'Number of decoder parameters: {sum(p.numel() for p in vae_gaussian.decoder.parameters())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "135305c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianEncoder(\n",
      "  (conv_head): ModuleList(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0, inplace=True)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ELU(alpha=1.0, inplace=True)\n",
      "  )\n",
      "  (mu): Linear(in_features=2048, out_features=50, bias=True)\n",
      "  (var): Linear(in_features=2048, out_features=50, bias=True)\n",
      "  (var_act): Softplus(beta=1.0, threshold=20.0)\n",
      ")\n",
      "GaussianDecoder(\n",
      "  (linear): Linear(in_features=50, out_features=2048, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ELU(alpha=1.0, inplace=True)\n",
      "    (3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ELU(alpha=1.0, inplace=True)\n",
      "    (6): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  )\n",
      ")\n",
      "Number of encoder parameters: 298020\n",
      "Number of decoder parameters: 197185\n"
     ]
    }
   ],
   "source": [
    "vae_gaussian = GaussianVAE(num_latent, train_D)\n",
    "print_vae_params(vae_gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1623497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, threshold: float, patience=3):\n",
    "        \"\"\"\n",
    "        :param threshold: The loss decrease threshold to consider for early stopping\n",
    "        :param patience: The number of epochs to wait before stopping\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "        self.patience = patience\n",
    "        self.last_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, loss: float) -> bool:\n",
    "        \"\"\"\n",
    "        :param loss: The current loss\n",
    "        :return: True if the training should stop, False otherwise\n",
    "        \"\"\"\n",
    "        if loss >= self.last_loss or self.last_loss - loss < self.threshold:\n",
    "            self.counter += 1\n",
    "        else:\n",
    "            self.counter = 0\n",
    "        self.last_loss = loss\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "822e1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "\n",
    "def load_weights(model: Module, dist_type: str, model_type: str):\n",
    "    model.load_state_dict(pickle.load(open(f\"models/{dist_type}/{model_type}.pkl\", \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b78062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def train_vae(\n",
    "    vae: GaussianVAE, \n",
    "    train_x: torch.Tensor, \n",
    "    #val_x: torch.Tensor, \n",
    "    stop_early, \n",
    "    learning_rate: float,\n",
    "    batch_size: int,\n",
    "    num_epochs: int,\n",
    "    img_size: int,\n",
    "    dist_type: str = \"gaussian\"\n",
    ") -> Tuple[List, List]:\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    vae.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(vae.encoder.parameters()) + list(vae.decoder.parameters()), \n",
    "        lr=learning_rate\n",
    "    )\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "\n",
    "    for epoch in pbar:\n",
    "        vae.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        # Shuffle indices for training data\n",
    "        shuffled_idx = torch.randperm(train_x.shape[0])\n",
    "        idx_batches = torch.split(shuffled_idx, batch_size)\n",
    "\n",
    "        for idx in idx_batches:\n",
    "            optimizer.zero_grad()\n",
    "            batch_x = train_x[idx].to(device)\n",
    "\n",
    "            # Reshape batch_x to (batch_size, channels=1, height, width)\n",
    "            input_x = batch_x.view(-1, 1, img_size, img_size)\n",
    "\n",
    "            loss = vae.elbo(input_x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        train_loss /= train_x.shape[0]\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        vae.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            shuffled_idx = torch.randperm(val_x.shape[0])\n",
    "            idx_batches = torch.split(shuffled_idx, batch_size)\n",
    "\n",
    "            for idx in idx_batches:\n",
    "                batch_x = val_x[idx].to(device)\n",
    "                input_x = batch_x.view(-1, 1, img_size, img_size)\n",
    "\n",
    "                loss = vae.elbo(input_x)\n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            val_loss /= val_x.shape[0]\n",
    "            val_loss_history.append(val_loss)\n",
    "\n",
    "        pbar.set_postfix({'Train Loss': train_loss, 'Validation Loss': val_loss})\n",
    "\n",
    "        if stop_early(train_loss_history[-1]):\n",
    "            print(f'Training criterion reached at epoch {epoch}. Stopping training...')\n",
    "            # Save model weights\n",
    "            torch.save(vae.encoder.state_dict(), f\"models/{dist_type}/encoder.pth\")\n",
    "            torch.save(vae.decoder.state_dict(), f\"models/{dist_type}/decoder.pth\")\n",
    "            break\n",
    "\n",
    "    return train_loss_history, val_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9422124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a new Gaussian VAE...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_vae() missing 1 required positional argument: 'val_x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining a new Gaussian VAE...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m EarlyStopping(stop_threshold)\n\u001b[1;32m----> 8\u001b[0m gaussian_train_loss, gaussian_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_vae\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_gaussian\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# o train_x, a seconda di come li chiami tu\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#val_x=val_dataset,          # o val_x\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_early\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_early\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdist_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_type\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: train_vae() missing 1 required positional argument: 'val_x'"
     ]
    }
   ],
   "source": [
    "if load_models:\n",
    "    load_weights(vae_gaussian.encoder, dist_type, 'encoder')\n",
    "    load_weights(vae_gaussian.decoder, dist_type, 'decoder')\n",
    "else:\n",
    "    print('Training a new Gaussian VAE...')\n",
    "    stop_early = EarlyStopping(stop_threshold)\n",
    "\n",
    "    gaussian_train_loss, gaussian_val_loss = train_vae(\n",
    "        vae=vae_gaussian,\n",
    "        train_x=train_dataset,      # o train_x, a seconda di come li chiami tu\n",
    "        #val_x=val_dataset,          # o val_x\n",
    "        stop_early=stop_early,\n",
    "        learning_rate=learning_rate,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=num_epochs,\n",
    "        img_size=28,\n",
    "        dist_type=dist_type\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
