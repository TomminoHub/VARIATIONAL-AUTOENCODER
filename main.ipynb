{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fb07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tommaso\\miniconda3\\envs\\genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import datasets\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "#datasets.mkdir_p('img_vae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f359c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent = 50\n",
    "num_bins = 51\n",
    "num_neurons = [1000, 1000]\n",
    "batch_size = 50\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4\n",
    "load_models = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "159fb555",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b35a80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [01:44<00:00, 95.2kB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 146kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:20<00:00, 79.2kB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb60ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "469\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e09ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_x = train_x[-10000:, :]\n",
    "train_x = train_x[:-10000, :]\n",
    "valid_labels = train_labels[-10000:]\n",
    "train_labels = train_labels[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b03597",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.tensor(train_x).to(device)\n",
    "test_x = torch.tensor(test_x).to(device)\n",
    "val_x = torch.tensor(valid_x).to(device)\n",
    "\n",
    "\n",
    "train_N, train_D = train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7d834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of example: 60000\n",
      "dimension of flat image: 784\n"
     ]
    }
   ],
   "source": [
    "train_N = len(train_dataset)\n",
    "\n",
    "first_img, _ = train_dataset[0]\n",
    "train_D = first_img.numel() \n",
    "\n",
    "print(f\"number of example: {train_N}\")\n",
    "print(f\"dimension of flat image: {train_D}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fad1a74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, in_channels: int = 1, n_conv_blocks: int = 2, base_filters: int = 32):\n",
    "        \"\"\"\n",
    "        Encoder che usa convoluzioni e restituisce media e logvar per la distribuzione latente q(z|x)\n",
    "        \n",
    "        Args:\n",
    "            n_latent (int): dimensione del vettore latente Z\n",
    "            in_channels (int): numero di canali dell'immagine (1 per MNIST)\n",
    "            n_conv_blocks (int): numero di blocchi convoluzionali\n",
    "            base_filters (int): numero iniziale di filtri (verrà raddoppiato a ogni blocco)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        filters = base_filters\n",
    "\n",
    "        # Costruzione dei blocchi convoluzionali\n",
    "        for _ in range(n_conv_blocks):\n",
    "            layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, stride=2, padding=1))\n",
    "            layers.append(nn.BatchNorm2d(filters))\n",
    "            layers.append(nn.ELU(inplace=True))\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "        # Calcola dimensione dell'output convoluzionale\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 1, 28, 28)  # immagine finta MNIST\n",
    "            conv_out = self.conv(dummy_input)\n",
    "            self.flattened_size = conv_out.view(1, -1).shape[1]\n",
    "\n",
    "        # Strato finale che mappa su media e logvar del vettore latente\n",
    "        self.fc_mu = nn.Linear(self.flattened_size, n_latent)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_size, n_latent)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8978a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_latent: int, output_channels: int = 1, n_conv_blocks: int = 2, base_filters: int = 32):\n",
    "        \"\"\"\n",
    "        Decoder che prende un vettore latente z e ricostruisce un'immagine MNIST (28x28).\n",
    "        \n",
    "        Args:\n",
    "            n_latent (int): dimensione del vettore latente z\n",
    "            output_channels (int): numero di canali dell'immagine in output (1 per MNIST)\n",
    "            n_conv_blocks (int): numero di blocchi deconvoluzionali\n",
    "            base_filters (int): numero iniziale di filtri (decrescente durante il decoding)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_conv_blocks = n_conv_blocks\n",
    "        self.base_filters = base_filters\n",
    "        self.output_channels = output_channels\n",
    "\n",
    "        # Calcolo della dimensione della feature map dopo l'encoder (dipende dalla struttura simmetrica)\n",
    "        self.init_spatial_dim = 28 // (2 ** n_conv_blocks)  # per MNIST 28x28\n",
    "        self.init_filters = base_filters * (2 ** (n_conv_blocks - 1))\n",
    "        self.projected_dim = self.init_filters * self.init_spatial_dim * self.init_spatial_dim\n",
    "\n",
    "        # Proiezione lineare dal vettore latente alla mappa iniziale\n",
    "        self.fc = nn.Linear(n_latent, self.projected_dim)\n",
    "\n",
    "        # Costruzione dei blocchi trasposti\n",
    "        layers = []\n",
    "        filters = self.init_filters\n",
    "        for i in range(n_conv_blocks - 1):\n",
    "            layers.append(nn.ConvTranspose2d(filters, filters // 2, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "            layers.append(nn.BatchNorm2d(filters // 2))\n",
    "            layers.append(nn.ELU(inplace=True))\n",
    "            filters //= 2\n",
    "\n",
    "        # Ultimo blocco: output a 1 canale (immagine ricostruita)\n",
    "        layers.append(nn.ConvTranspose2d(filters, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1))\n",
    "        layers.append(nn.Sigmoid())  # output in [0, 1]\n",
    "\n",
    "        self.deconv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)\n",
    "        x = x.view(x.size(0), self.init_filters, self.init_spatial_dim, self.init_spatial_dim)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd589a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mVAE\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder: nn\u001b[38;5;241m.\u001b[39mModule, decoder: nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.encoder = encoder.to(self.device)\n",
    "        self.decoder = decoder.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        VAE: encode → reparametrize → decode.\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "         z = mu + sigma * epsilon\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        return (mu, logvar)\n",
    "        \"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Reconstruct the image starting from latent vector z\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def reconstruct(self, x):\n",
    "        \"\"\"\n",
    "        encode → reparametrize → decode\n",
    "        \"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z)\n",
    "\n",
    "    def sample(self, n_samples: int):\n",
    "        \"\"\"\n",
    "        sample z ~ N(0,I) .\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(n_samples, self.encoder.n_latent, device=self.device)\n",
    "            samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def elbo(self, x, beta=1.0):\n",
    "        \"\"\"\n",
    "        Compute ELBO\n",
    "        \"\"\"\n",
    "        x_recon, mu, logvar = self.forward(x)\n",
    "\n",
    "        # Ricostruzione con MSE o BCE\n",
    "        recon_loss = nn.functional.binary_cross_entropy(\n",
    "            x_recon, x, reduction='sum'\n",
    "        )\n",
    "\n",
    "        # KL divergente tra N(mu, sigma^2) e N(0, 1)\n",
    "        kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "        elbo = recon_loss + beta * kl_div\n",
    "        return elbo, recon_loss, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0bfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
