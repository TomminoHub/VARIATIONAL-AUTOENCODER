# VARIATIONAL-AUTOENCODER
This project implements and explores Variational Autoencoders (VAEs) using PyTorch, focusing on both model design and analysis of the latent space. The encoder and decoder are built with convolutional layers to learn compact representations of MNIST digits. The model is trained using the ELBO objective, first assuming a Gaussian likelihood, then experimenting with a Beta distribution to model pixel values. The project includes various evaluations such as reconstruction quality, image generation, and visualization of the latent space using PCA. It also explores how digits can be smoothly interpolated in the latent space and shows how to reconstruct images without using the encoder, including partial input completion. The goal is to understand the generative capabilities of VAEs and how different output distributions affect performance and visual quality.
